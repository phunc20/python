{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"013592e6310343e8b3f34283230446d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_750274b61be34e6faac73a3d202d9f7a","placeholder":"​","style":"IPY_MODEL_ead703e085274880980ad566d3bf9687","value":"Downloading data files: 100%"}},"05b6c4ec0da749498e8b1f634c0d2e00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b7c56489eb2400c89d5511c81268369":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a62e7fc52f374301b5dbb2edcee5bc73","placeholder":"​","style":"IPY_MODEL_619f83b36beb49daa2abf11d9efd8ae6","value":" 365306/0 [00:15&lt;00:00, 15831.00 examples/s]"}},"0f649caa7061461aa251e52460e7c20b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c98731b805f497196ff020c4154b0a2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_650a1a4e71eb48cc8057ee1fb3332c7e","value":1}},"107696b3b68e48fbaeb122d8e71f3507":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1339fcb062a543aab97e6d30679c669e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"145e6d62c4f54f9fb62943abd3cac34b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e674c5ebfa549f889a5616b19deb3fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_444525a356034eca9140497e8f3d9e35","placeholder":"​","style":"IPY_MODEL_264ad5a3138f404dab02e95379e1fe66","value":"Generating train split: "}},"264ad5a3138f404dab02e95379e1fe66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"268cd8fb1b644899a41fbb861759d999":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a4dc65fba214f069603a1162bacacce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2cec918c950b45aea55d13edc700eac4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e6452f7f9554fafbf96a824e6330d95":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f9a058ffab746dcbb83303858653666":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"329038d9aa68464ba89228cdc7e4603c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36c6e5d516844969b2bd61d881a912c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38dc22670e69459dbcd94a2942451d3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cec918c950b45aea55d13edc700eac4","placeholder":"​","style":"IPY_MODEL_994acfe916874724b564d93c6d63d22a","value":" 1/1 [00:00&lt;00:00, 42.14it/s]"}},"390ada49ea604828a6ca3d450fac66af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1e674c5ebfa549f889a5616b19deb3fa","IPY_MODEL_a62ab2e0414c4e208bcbbbebee16181e","IPY_MODEL_0b7c56489eb2400c89d5511c81268369"],"layout":"IPY_MODEL_4cbf18c45a74432793d9c181de3a9e54"}},"3e11a96e1d03437a88cb5cc60eb78e5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c65dcc07f7ef45aa8b5662501a2081ef","placeholder":"​","style":"IPY_MODEL_329038d9aa68464ba89228cdc7e4603c","value":" 1/1 [00:03&lt;00:00,  3.07s/it]"}},"444525a356034eca9140497e8f3d9e35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cbf18c45a74432793d9c181de3a9e54":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"5704dab3c4dd4b299f2d13be36cf3bfd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"582cafc2454f4e97bdff387864b1f773":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c07741feb534dbf999cb2fcda747f94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"619f83b36beb49daa2abf11d9efd8ae6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6230e6e184334f3089c3a14534cfc20d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"650a1a4e71eb48cc8057ee1fb3332c7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f422f9c6d504471921cb85c058cf546":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"750274b61be34e6faac73a3d202d9f7a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7568c2f964d44172b633bdf67582169e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c07741feb534dbf999cb2fcda747f94","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6230e6e184334f3089c3a14534cfc20d","value":1}},"7c98731b805f497196ff020c4154b0a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e3d8839a8d7487cb96e11ad926fa28e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"994acfe916874724b564d93c6d63d22a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a62ab2e0414c4e208bcbbbebee16181e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_b679f33cd8ca482abfe1d73a8d84f221","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a4dc65fba214f069603a1162bacacce","value":1}},"a62e7fc52f374301b5dbb2edcee5bc73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8ec87f622ab449ca0254ddef6f52129":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"add370357eda43a9afaefbd4756693ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0e2f5c15db6453fac89bc2a6941919b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05b6c4ec0da749498e8b1f634c0d2e00","placeholder":"​","style":"IPY_MODEL_add370357eda43a9afaefbd4756693ef","value":"100%"}},"b53228a47af846a1b7feed62c855cc58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0e2f5c15db6453fac89bc2a6941919b","IPY_MODEL_edd9f7f4496348b08f134668fea2ca34","IPY_MODEL_e0bf9dfe67154caf99502f8a041c9b30"],"layout":"IPY_MODEL_2e6452f7f9554fafbf96a824e6330d95"}},"b679f33cd8ca482abfe1d73a8d84f221":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"b9ba1cbd9fc24b359a88653f8725e75b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5704dab3c4dd4b299f2d13be36cf3bfd","placeholder":"​","style":"IPY_MODEL_107696b3b68e48fbaeb122d8e71f3507","value":"Extracting data files: 100%"}},"bcc6091dfb98439e933df182c1caffd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f09f7485fe7543f89cba9f6f36840fbd","IPY_MODEL_7568c2f964d44172b633bdf67582169e","IPY_MODEL_e78c096363a34aa99313a2ed91013753"],"layout":"IPY_MODEL_8e3d8839a8d7487cb96e11ad926fa28e"}},"c65dcc07f7ef45aa8b5662501a2081ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cce51176f28e4a1480cb3bcb755c6d61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9ba1cbd9fc24b359a88653f8725e75b","IPY_MODEL_0f649caa7061461aa251e52460e7c20b","IPY_MODEL_3e11a96e1d03437a88cb5cc60eb78e5a"],"layout":"IPY_MODEL_cf31455adbcc47598a09385fb128be5d"}},"cf31455adbcc47598a09385fb128be5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5a559e3893c417cacf17b232cc3f860":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_013592e6310343e8b3f34283230446d6","IPY_MODEL_e625aee80e60480fa3d232f3877f8df8","IPY_MODEL_38dc22670e69459dbcd94a2942451d3c"],"layout":"IPY_MODEL_145e6d62c4f54f9fb62943abd3cac34b"}},"e0348e3d40704ad0805746778db5f379":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0bf9dfe67154caf99502f8a041c9b30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f9a058ffab746dcbb83303858653666","placeholder":"​","style":"IPY_MODEL_36c6e5d516844969b2bd61d881a912c9","value":" 1/1 [00:00&lt;00:00, 19.82it/s]"}},"e625aee80e60480fa3d232f3877f8df8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f422f9c6d504471921cb85c058cf546","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f71ae87e3b7f4a9e95c5001cf1835455","value":1}},"e78c096363a34aa99313a2ed91013753":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f783430f9ccf495bbb38bccf97b97019","placeholder":"​","style":"IPY_MODEL_582cafc2454f4e97bdff387864b1f773","value":" 365306/0 [16:51&lt;00:00, 202.56 examples/s]"}},"ead703e085274880980ad566d3bf9687":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edd9f7f4496348b08f134668fea2ca34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_268cd8fb1b644899a41fbb861759d999","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8ec87f622ab449ca0254ddef6f52129","value":1}},"f09f7485fe7543f89cba9f6f36840fbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0348e3d40704ad0805746778db5f379","placeholder":"​","style":"IPY_MODEL_1339fcb062a543aab97e6d30679c669e","value":"Generating train split: "}},"f71ae87e3b7f4a9e95c5001cf1835455":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f783430f9ccf495bbb38bccf97b97019":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%bash\npip install -q datasets==2.10.1 transformers==4.26.1 tokenizers==0.12.1\npip list | grep --color \"^dataset\\|^transfor\\|^tokeniz\"","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:52:35.991234Z","iopub.execute_input":"2023-06-26T14:52:35.991864Z","iopub.status.idle":"2023-06-26T14:53:01.234084Z","shell.execute_reply.started":"2023-06-26T14:52:35.991828Z","shell.execute_reply":"2023-06-26T14:53:01.233092Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"},{"name":"stdout","text":"datasets                               2.10.1\ntokenizers                             0.12.1\ntransformers                           4.26.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:53:01.235963Z","iopub.execute_input":"2023-06-26T14:53:01.236300Z","iopub.status.idle":"2023-06-26T14:53:04.059021Z","shell.execute_reply.started":"2023-06-26T14:53:01.236265Z","shell.execute_reply":"2023-06-26T14:53:04.058064Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"USE_CUDA = torch.cuda.is_available()\n#USE_CUDA = False\nBATCH_SIZE = 64\n#MAX_STEPS = 300\nFP16 = USE_CUDA\nDEVICE = torch.device(\"cuda\") if USE_CUDA else torch.device(\"cpu\")\n#DEVICE = torch.device(\"cpu\")\nprint(f'{USE_CUDA   = }')\nprint(f'{BATCH_SIZE = }')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T15:02:10.099011Z","iopub.execute_input":"2023-06-26T15:02:10.099377Z","iopub.status.idle":"2023-06-26T15:02:10.111040Z","shell.execute_reply.started":"2023-06-26T15:02:10.099346Z","shell.execute_reply":"2023-06-26T15:02:10.109918Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"USE_CUDA   = True\nBATCH_SIZE = 64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Tokenizer","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Now let's load our tokenizer from Hugging Face Hub","metadata":{"id":"yAwQ82JiE5pi"}},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast\n\ntokenizer_repo = \"phunc20/esperoberta-cased\"\ntokenizer = RobertaTokenizerFast.from_pretrained(\n    tokenizer_repo,\n)\ntokenizer.model_max_length","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4keFBUjQFOD1","outputId":"7267e1f9-dd7f-4920-de99-b4a336c317b2","tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:53:09.005682Z","iopub.execute_input":"2023-06-26T14:53:09.006065Z","iopub.status.idle":"2023-06-26T14:53:18.875483Z","shell.execute_reply.started":"2023-06-26T14:53:09.006033Z","shell.execute_reply":"2023-06-26T14:53:18.874357Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/311 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8b44ede02bf4741a82ec96ded645686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/478k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eda91f79edeb491fa055189f9ecb593c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/279k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4069e39e158c40d7930591b5544f742d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5be92293c6e14431a3c4d331b8d62909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"578b4ea34d554f54944c2a2306bdaada"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}]},{"cell_type":"markdown","source":"**(?)** Is it normal to have such a big `tokenizer.model_max_length`?  \n","metadata":{}},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:53:18.877596Z","iopub.execute_input":"2023-06-26T14:53:18.878338Z","iopub.status.idle":"2023-06-26T14:53:18.889047Z","shell.execute_reply.started":"2023-06-26T14:53:18.878304Z","shell.execute_reply":"2023-06-26T14:53:18.888153Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"30000"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:54:02.188834Z","iopub.execute_input":"2023-06-26T14:54:02.189196Z","iopub.status.idle":"2023-06-26T14:54:02.194586Z","shell.execute_reply.started":"2023-06-26T14:54:02.189168Z","shell.execute_reply":"2023-06-26T14:54:02.193658Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Load The Dataset","metadata":{"tags":[]}},{"cell_type":"code","source":"from pathlib import Path\n\n# dataset_dir = Path(\"/kaggle/input/vina-cased-chunked-dataset\")\n# dataset_dir.exists()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:09.302937Z","iopub.execute_input":"2023-06-26T14:54:09.303478Z","iopub.status.idle":"2023-06-26T14:54:09.309588Z","shell.execute_reply.started":"2023-06-26T14:54:09.303442Z","shell.execute_reply":"2023-06-26T14:54:09.307596Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:09.849549Z","iopub.execute_input":"2023-06-26T14:54:09.850486Z","iopub.status.idle":"2023-06-26T14:54:09.855469Z","shell.execute_reply.started":"2023-06-26T14:54:09.850440Z","shell.execute_reply":"2023-06-26T14:54:09.854469Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# from huggingface_hub import notebook_login\n\n# notebook_login()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:13.603440Z","iopub.execute_input":"2023-06-26T14:54:13.603823Z","iopub.status.idle":"2023-06-26T14:54:13.608020Z","shell.execute_reply.started":"2023-06-26T14:54:13.603784Z","shell.execute_reply":"2023-06-26T14:54:13.607065Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"dataset_repo = \"phunc20/oscar_esperoberta-cased_dataset\"\ndataset = load_dataset(dataset_repo)\ndataset","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:14.031657Z","iopub.execute_input":"2023-06-26T14:54:14.032350Z","iopub.status.idle":"2023-06-26T14:54:14.982565Z","shell.execute_reply.started":"2023-06-26T14:54:14.032315Z","shell.execute_reply":"2023-06-26T14:54:14.981657Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d450887954fe487488f65935aa4698cf"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 295376\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Model","metadata":{"id":"u0qQzgrBi1OX","tags":[]}},{"cell_type":"code","source":"from transformers import RobertaConfig","metadata":{"id":"LTXXutqeDzPi","tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:18.960513Z","iopub.execute_input":"2023-06-26T14:54:18.961489Z","iopub.status.idle":"2023-06-26T14:54:18.971161Z","shell.execute_reply.started":"2023-06-26T14:54:18.961455Z","shell.execute_reply":"2023-06-26T14:54:18.970110Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"raw","source":"RobertaConfig?","metadata":{"tags":[]}},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:19.557991Z","iopub.execute_input":"2023-06-26T14:54:19.558886Z","iopub.status.idle":"2023-06-26T14:54:19.564785Z","shell.execute_reply.started":"2023-06-26T14:54:19.558849Z","shell.execute_reply":"2023-06-26T14:54:19.563807Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"30000"},"metadata":{}}]},{"cell_type":"raw","source":"# Default\nRobertaConfig(\n    vocab_size=30522,\n    hidden_size=768,\n    num_hidden_layers=12,\n    num_attention_heads=12,\n    intermediate_size=3072,\n    hidden_act='gelu',\n    hidden_dropout_prob=0.1,\n    attention_probs_dropout_prob=0.1,\n    max_position_embeddings=512,\n    type_vocab_size=2,\n    initializer_range=0.02,\n    layer_norm_eps=1e-12,\n    pad_token_id=1,\n    bos_token_id=0,\n    eos_token_id=2,\n    position_embedding_type='absolute',\n    use_cache=True,\n    classifier_dropout=None,\n    **kwargs,\n)","metadata":{}},{"cell_type":"code","source":"tokenizer.bos_token_id, tokenizer.eos_token_id","metadata":{"tags":[]},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":["(0, 2)"]},"metadata":{}}]},{"cell_type":"code","source":"config = RobertaConfig(\n    vocab_size=tokenizer.vocab_size,\n    #max_position_embeddings=514,\n    #num_attention_heads=12,\n    #num_hidden_layers=6,\n    #type_vocab_size=1,\n)","metadata":{"id":"LTXXutqeDzPi","tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:25.882843Z","iopub.execute_input":"2023-06-26T14:54:25.883275Z","iopub.status.idle":"2023-06-26T14:54:25.888621Z","shell.execute_reply.started":"2023-06-26T14:54:25.883240Z","shell.execute_reply":"2023-06-26T14:54:25.887748Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**(?)** `type_vocab_size`? What is it?  ","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaForMaskedLM\n\nmodel = RobertaForMaskedLM(config=config)\nprint(f'{model.num_parameters():,d}')  # ~84 million parameters","metadata":{"id":"BzMqR-dzF4Ro","tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:28.791740Z","iopub.execute_input":"2023-06-26T14:54:28.792185Z","iopub.status.idle":"2023-06-26T14:54:32.620370Z","shell.execute_reply.started":"2023-06-26T14:54:28.792148Z","shell.execute_reply":"2023-06-26T14:54:32.619240Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"109,112,880\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's test its ability to guess masked sentences **before training**.","metadata":{}},{"cell_type":"raw","source":"import torch\n\ntext = f'Di may {tokenizer.mask_token} sang My.'\nencoding = tokenizer(text, return_tensors=\"pt\")\nprint(f'{type(encoding) = }')\nprint(f'{encoding = }')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jU6JhBSTKiaM","outputId":"217c9f98-f292-46be-f0c0-07ef67267eb5","tags":[]}},{"cell_type":"raw","source":"output = model(**encoding)\nprint(f'{output = }')","metadata":{"tags":[]}},{"cell_type":"raw","source":"token_logits = output.logits\ntoken_logits.shape","metadata":{"tags":[]}},{"cell_type":"raw","source":"tokenizer.vocab_size == token_logits.shape[-1]","metadata":{"tags":[]}},{"cell_type":"raw","source":"torch.argmax(token_logits, dim=-1).shape","metadata":{"tags":[]}},{"cell_type":"raw","source":"tokenizer.convert_ids_to_tokens(torch.argmax(token_logits, dim=-1)[0])","metadata":{"tags":[]}},{"cell_type":"raw","source":"# Find the location of <mask> and extract its logits\nmask_token_index = torch.where(encoding[\"input_ids\"] == tokenizer.mask_token_id)[1]\nmask_token_logits = token_logits[0, mask_token_index, :]\n# Pick the <mask> candidates with the highest logits\ntop_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n\nfor token in top_5_tokens:\n    print(f\">>> '{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")","metadata":{"tags":[]}},{"cell_type":"code","source":"from transformers import pipeline\n\nfill_mask = pipeline(\n      \"fill-mask\",\n      model=model,\n      tokenizer=tokenizer,\n)\n# En 1831, havante 22 jarojn\nresult = fill_mask(\"\"\"\\\nEn 1831, havante 22 <mask>, ŝatanto de skaraboj sen direkto en vivo,\nDarvino vojaĝis ĉirkaŭ la Tero en la ŝipo HMS Beagle,\ndum kvin jaroj.\\\n\"\"\")\nresult","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:37.986638Z","iopub.execute_input":"2023-06-26T14:54:37.986988Z","iopub.status.idle":"2023-06-26T14:54:40.208545Z","shell.execute_reply.started":"2023-06-26T14:54:37.986959Z","shell.execute_reply":"2023-06-26T14:54:40.207557Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[{'score': 0.00030039899866096675,\n  'token': 28240,\n  'token_str': ' Kart',\n  'sequence': 'En 1831, havante 22 Kart, ŝatanto de skaraboj sen direkto en vivo, Darvino vojaĝis ĉirkaŭ la Tero en la ŝipo HMS Beagle, dum kvin jaroj.'},\n {'score': 0.0002480414404999465,\n  'token': 10883,\n  'token_str': ' 1947',\n  'sequence': 'En 1831, havante 22 1947, ŝatanto de skaraboj sen direkto en vivo, Darvino vojaĝis ĉirkaŭ la Tero en la ŝipo HMS Beagle, dum kvin jaroj.'},\n {'score': 0.00023199191491585225,\n  'token': 25307,\n  'token_str': 'naskiĝinta',\n  'sequence': 'En 1831, havante 22naskiĝinta, ŝatanto de skaraboj sen direkto en vivo, Darvino vojaĝis ĉirkaŭ la Tero en la ŝipo HMS Beagle, dum kvin jaroj.'},\n {'score': 0.0002099921548506245,\n  'token': 15543,\n  'token_str': 'ĥu',\n  'sequence': 'En 1831, havante 22ĥu, ŝatanto de skaraboj sen direkto en vivo, Darvino vojaĝis ĉirkaŭ la Tero en la ŝipo HMS Beagle, dum kvin jaroj.'},\n {'score': 0.00020650227088481188,\n  'token': 2599,\n  'token_str': 'Hel',\n  'sequence': 'En 1831, havante 22Hel, ŝatanto de skaraboj sen direkto en vivo, Darvino vojaĝis ĉirkaŭ la Tero en la ŝipo HMS Beagle, dum kvin jaroj.'}]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Collator","metadata":{"tags":[]}},{"cell_type":"raw","source":"import collections\n\nimport numpy as np\nfrom transformers import default_data_collator\n\nwwm_probability = 0.2\n\n\ndef whole_word_masking_data_collator(features):\n    for feature in features:\n        word_ids = feature.pop(\"word_ids\")\n\n        # Create a map between words and corresponding token indices\n        mapping = collections.defaultdict(list)\n        current_word_index = -1\n        current_word = None\n        for idx, word_id in enumerate(word_ids):\n            if word_id is not None:\n                if word_id != current_word:\n                    current_word = word_id\n                    current_word_index += 1\n                mapping[current_word_index].append(idx)\n\n        # Randomly mask words\n        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n        input_ids = feature[\"input_ids\"]\n        labels = feature[\"labels\"]\n        new_labels = [-100] * len(labels)\n        for word_id in np.where(mask)[0]:\n            word_id = word_id.item()\n            for idx in mapping[word_id]:\n                new_labels[idx] = labels[idx]\n                input_ids[idx] = tokenizer.mask_token_id\n        feature[\"labels\"] = new_labels\n\n    return default_data_collator(features)","metadata":{"tags":[]}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm_probability=0.15,\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:53.733546Z","iopub.execute_input":"2023-06-26T14:54:53.734575Z","iopub.status.idle":"2023-06-26T14:54:53.740472Z","shell.execute_reply.started":"2023-06-26T14:54:53.734529Z","shell.execute_reply":"2023-06-26T14:54:53.739150Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"> Data collator expects as input **a list of dictionaries**","metadata":{}},{"cell_type":"code","source":"n_samples = 2\nsamples = [dataset[\"train\"][i] for i in range(n_samples)]\n# print(f'{samples = }')\nfor sample in samples:\n    #import ipdb; ipdb.set_trace()\n    del sample[\"word_ids\"]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:54:58.528446Z","iopub.execute_input":"2023-06-26T14:54:58.529132Z","iopub.status.idle":"2023-06-26T14:54:58.538253Z","shell.execute_reply.started":"2023-06-26T14:54:58.529099Z","shell.execute_reply":"2023-06-26T14:54:58.537153Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"collated_samples = data_collator(samples)\ncollated_samples.keys()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:55:00.066138Z","iopub.execute_input":"2023-06-26T14:55:00.066849Z","iopub.status.idle":"2023-06-26T14:55:00.082790Z","shell.execute_reply.started":"2023-06-26T14:55:00.066815Z","shell.execute_reply":"2023-06-26T14:55:00.081842Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'attention_mask', 'labels'])"},"metadata":{}}]},{"cell_type":"markdown","source":"Note that\n- In `input_ids` there are about `mlm_probability` ratio of mask ids, `tokenizer.mask_token_id`\n- In the corresponding position in `labels`, we can see the \"correct\" token id of the masked token. All the unmasked\n  positions have `-100` as label, i.e. to be ignored","metadata":{}},{"cell_type":"code","source":"for chunk, label in zip(collated_samples[\"input_ids\"],\n                        collated_samples[\"labels\"]):\n    gt = []\n    for c, l in zip(chunk, label):\n        if c is None:\n            continue\n        if l == -100:\n            gt.append(c)\n        else:\n            gt.append(l)\n    gt = tokenizer.decode(\n        gt,\n        skip_special_tokens=True,\n    )\n    print(f'(label) {gt}')\n    print()\n    print(f\"(chunk) '{tokenizer.decode(chunk)}'\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:55:03.645371Z","iopub.execute_input":"2023-06-26T14:55:03.646062Z","iopub.status.idle":"2023-06-26T14:55:03.660907Z","shell.execute_reply.started":"2023-06-26T14:55:03.646029Z","shell.execute_reply":"2023-06-26T14:55:03.659735Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"(label) Ĉu... preĝi | mediti | ricevi instigojn || kanti | muziki || informiĝi | legi | studi || prepari DiservonTemas pri kolekto de kristanaj kantoj, eldonita de Adolf Burkhardt inter 1974 kaj 1990 en dek kajeretoj. Ili estas reeldonitaj inter 1995 kaj 1998 de Bernhard Eichkorn en tri kajeroj, kies tria estas pliampleksigita per Dek Novaj Kantoj kaj suplemento, same de Adolf Burkhardt.En la dua kaj tria kajero oni adiciis 300 al la originaj kantonumeroj, por ke oni povu pli facile uzi la kajerojn kune kun la KELI-himnaro Adoru Kantante,\n\n(chunk) '<s>Ĉu... preĝi | mediti<mask> ricevi instigojn || kanti<mask> muziki ||<mask> | legi | studi || prepari<mask></s><s>Temas pri kolekto de<mask><mask>, eldonita<mask> Adolf Burkhardt inter 1974 kaj 1990 en dek kajeretoj. Ili estas reeldonitaj inter 1995 kaj 1998 de Bernhard<mask>korn en tri kajeroj, kies<mask> estas pliampleksigita per Dek Novaj Kantoj kaj suplemento<mask> same de Adolf Burk<mask>.</s><s>En la dua<mask> tria kajero oni<mask>ciis 300 al la originaj kantonumeroj, por ke oni povu pli facile uzi la kajerojn kune kun la KELI-himn<mask> Adoru Kantante,'\n(label)  kiu havas malpli ol 300 numerojn.Ni ĝojus, se iu trovus bonajn ekzemplerojn de la dek originaj kajeretoj kaj tempon por skani ankaŭ ilin. Bonvolu ekkontaktiĝi kun ni!Lerni Esperanton per telefono, novaĵoj Poŝtkarto 120 jaroj de fervojo Svitavy-Polička 189… T.n.migranta poŝtkarto el 1908 BK - Kongresa Biblioteko en Vaŝingtono 1- 910 BK - Nederlando- Esperanta elektra tramo en Hago (… La lernolibro \"Esperanto per rekta metodo\" jam en… IMG 7181 Nova poŝtkarto - \"Esperanto sur poŝtmarkoj kaj bil… Vizitu urbon Přerov - bildkarto Saluton\n\n(chunk) ' kiu havas malpli ol 300 numerojn.</s><s>Ni<mask>, se iu trovus bonajn<mask> de la dek originaj kajere<mask> kaj tempon por<mask>i ankaŭ ilin<mask> Bonvolu ekkon<mask>ktiĝi kun ni!</s><s>Lerni Esperanton per telefono, novaĵoj<mask>karto 120 jaroj de fervojo Svitavy-<mask>čka 189… T<mask>n.migranta komprenukarto el 1908 BK - Kongresa Biblioteko<mask> Vaŝingtono 1- 910 BK - Nederlando- Esperanta elektra tramo en Hago<mask> ĝerman La lernolibro \"Esperanto per rekta metodo supren jam en… IMG 7181 Nova poŝtkarto - \"Esperanto sur poŝtmarkoj kaj bil… Vizitu urbon Př<mask>v - bild<mask> Saluton'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**(?)** Why there are those `\"�\"`?\n\n**Rmk.** Also, you might be interested in making sure only Latin-alphabet tokens are masked because masking characters from other languages, say, Chinese, wouldn't really help with our ultimate (Vietnamese) task.","metadata":{}},{"cell_type":"code","source":"tokenizer.special_tokens_map","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:55:12.082951Z","iopub.execute_input":"2023-06-26T14:55:12.083383Z","iopub.status.idle":"2023-06-26T14:55:12.091039Z","shell.execute_reply.started":"2023-06-26T14:55:12.083345Z","shell.execute_reply":"2023-06-26T14:55:12.089954Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'bos_token': '<s>',\n 'eos_token': '</s>',\n 'unk_token': '<unk>',\n 'sep_token': '</s>',\n 'pad_token': '<pad>',\n 'cls_token': '<s>',\n 'mask_token': '<mask>'}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Downsampled Dataset","metadata":{"tags":[]}},{"cell_type":"code","source":"dataset = dataset.remove_columns([\"word_ids\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:55:25.105037Z","iopub.execute_input":"2023-06-26T14:55:25.105404Z","iopub.status.idle":"2023-06-26T14:55:25.116748Z","shell.execute_reply.started":"2023-06-26T14:55:25.105375Z","shell.execute_reply":"2023-06-26T14:55:25.115735Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:55:51.797637Z","iopub.execute_input":"2023-06-26T14:55:51.798746Z","iopub.status.idle":"2023-06-26T14:55:51.804840Z","shell.execute_reply.started":"2023-06-26T14:55:51.798701Z","shell.execute_reply":"2023-06-26T14:55:51.803955Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 295376\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"#train_size = 1_000\n#test_size = 100\n\ndownsampled_dataset = dataset[\"train\"].train_test_split(\n    #train_size=train_size,\n    #test_size=test_size,\n    test_size=0.01,\n    seed=42\n)\ndownsampled_dataset","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:56:10.649565Z","iopub.execute_input":"2023-06-26T14:56:10.649966Z","iopub.status.idle":"2023-06-26T14:56:10.781479Z","shell.execute_reply.started":"2023-06-26T14:56:10.649936Z","shell.execute_reply":"2023-06-26T14:56:10.780406Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 292422\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 2954\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## `TrainingArguments`","metadata":{"tags":[]}},{"cell_type":"code","source":"from transformers import TrainingArguments","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:56:17.592900Z","iopub.execute_input":"2023-06-26T14:56:17.593261Z","iopub.status.idle":"2023-06-26T14:56:17.598212Z","shell.execute_reply.started":"2023-06-26T14:56:17.593231Z","shell.execute_reply":"2023-06-26T14:56:17.597126Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"TrainingArguments?","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:56:17.977302Z","iopub.execute_input":"2023-06-26T14:56:17.977669Z","iopub.status.idle":"2023-06-26T14:56:18.079188Z","shell.execute_reply.started":"2023-06-26T14:56:17.977638Z","shell.execute_reply":"2023-06-26T14:56:18.077810Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mTrainingArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdo_eval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdo_predict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'no'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mper_device_eval_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mper_gpu_train_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mper_gpu_eval_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0meval_accumulation_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0meval_delay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mweight_decay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0madam_beta1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0madam_beta2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0madam_epsilon\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlr_scheduler_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchedulerType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mwarmup_ratio\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlog_level\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'passive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlog_level_replica\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'passive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlog_on_each_node\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlogging_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlogging_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlogging_first_step\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlogging_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msave_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msave_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msave_total_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msave_on_each_node\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mno_cuda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_mps_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdata_seed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mjit_mode_eval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_ipex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbf16\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfp16_opt_level\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'O1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mhalf_precision_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbf16_full_eval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfp16_full_eval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtf32\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlocal_rank\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mxpu_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtpu_num_cores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtpu_metrics_debug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdataloader_drop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0meval_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdataloader_num_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpast_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mremove_unused_columns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlabel_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mload_best_model_at_end\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mgreater_is_better\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mignore_data_skip\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msharded_ddp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfsdp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfsdp_min_num_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfsdp_transformer_layer_cls_to_wrap\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0moptim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizerNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adamw_hf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0moptim_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0madafactor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mgroup_by_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlength_column_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreport_to\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mddp_find_unused_parameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mddp_bucket_cap_mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdataloader_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mskip_memory_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_legacy_prediction_loop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mhub_model_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mhub_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHubStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'every_save'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mhub_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mhub_private_repo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mgradient_checkpointing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0minclude_inputs_for_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfp16_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub_model_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub_organization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmp_parameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfull_determinism\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtorchdynamo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mray_scope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'last'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mddp_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtorch_compile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtorch_compile_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtorch_compile_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nTrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\nitself**.\n\nUsing [`HfArgumentParser`] we can turn this class into\n[argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\ncommand line.\n\nParameters:\n    output_dir (`str`):\n        The output directory where the model predictions and checkpoints will be written.\n    overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n        If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n        points to a checkpoint directory.\n    do_train (`bool`, *optional*, defaults to `False`):\n        Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n        by your training/evaluation scripts instead. See the [example\n        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n    do_eval (`bool`, *optional*):\n        Whether to run evaluation on the validation set or not. Will be set to `True` if `evaluation_strategy` is\n        different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n        training/evaluation scripts instead. See the [example\n        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n    do_predict (`bool`, *optional*, defaults to `False`):\n        Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n        intended to be used by your training/evaluation scripts instead. See the [example\n        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n    evaluation_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n        The evaluation strategy to adopt during training. Possible values are:\n\n            - `\"no\"`: No evaluation is done during training.\n            - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n            - `\"epoch\"`: Evaluation is done at the end of each epoch.\n\n    prediction_loss_only (`bool`, *optional*, defaults to `False`):\n        When performing evaluation and generating predictions, only returns the loss.\n    per_device_train_batch_size (`int`, *optional*, defaults to 8):\n        The batch size per GPU/TPU core/CPU for training.\n    per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n        The batch size per GPU/TPU core/CPU for evaluation.\n    gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n        Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n\n        <Tip warning={true}>\n\n        When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n        evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n\n        </Tip>\n\n    eval_accumulation_steps (`int`, *optional*):\n        Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n        left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but\n        requires more memory).\n    eval_delay (`float`, *optional*):\n        Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n        evaluation_strategy.\n    learning_rate (`float`, *optional*, defaults to 5e-5):\n        The initial learning rate for [`AdamW`] optimizer.\n    weight_decay (`float`, *optional*, defaults to 0):\n        The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n        optimizer.\n    adam_beta1 (`float`, *optional*, defaults to 0.9):\n        The beta1 hyperparameter for the [`AdamW`] optimizer.\n    adam_beta2 (`float`, *optional*, defaults to 0.999):\n        The beta2 hyperparameter for the [`AdamW`] optimizer.\n    adam_epsilon (`float`, *optional*, defaults to 1e-8):\n        The epsilon hyperparameter for the [`AdamW`] optimizer.\n    max_grad_norm (`float`, *optional*, defaults to 1.0):\n        Maximum gradient norm (for gradient clipping).\n    num_train_epochs(`float`, *optional*, defaults to 3.0):\n        Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n        the last epoch before stopping training).\n    max_steps (`int`, *optional*, defaults to -1):\n        If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n        In case of using a finite iterable dataset the training may stop before reaching the set number of steps\n        when all data is exhausted\n    lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n        The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n    warmup_ratio (`float`, *optional*, defaults to 0.0):\n        Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n    warmup_steps (`int`, *optional*, defaults to 0):\n        Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n    log_level (`str`, *optional*, defaults to `passive`):\n        Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n        'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and lets the\n        application set the level.\n    log_level_replica (`str`, *optional*, defaults to `passive`):\n        Logger log level to use on replicas. Same choices as `log_level`\"\n    log_on_each_node (`bool`, *optional*, defaults to `True`):\n        In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n        node.\n    logging_dir (`str`, *optional*):\n        [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n        *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n    logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n        The logging strategy to adopt during training. Possible values are:\n\n            - `\"no\"`: No logging is done during training.\n            - `\"epoch\"`: Logging is done at the end of each epoch.\n            - `\"steps\"`: Logging is done every `logging_steps`.\n\n    logging_first_step (`bool`, *optional*, defaults to `False`):\n        Whether to log and evaluate the first `global_step` or not.\n    logging_steps (`int`, *optional*, defaults to 500):\n        Number of update steps between two logs if `logging_strategy=\"steps\"`.\n    logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n        Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n        or `inf` is filtered and the average loss of the current logging window is taken instead.\n\n        <Tip>\n\n        `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n        gradient is computed or applied to the model.\n\n        </Tip>\n\n    save_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n        The checkpoint save strategy to adopt during training. Possible values are:\n\n            - `\"no\"`: No save is done during training.\n            - `\"epoch\"`: Save is done at the end of each epoch.\n            - `\"steps\"`: Save is done every `save_steps`.\n    save_steps (`int`, *optional*, defaults to 500):\n        Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`.\n    save_total_limit (`int`, *optional*):\n        If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n        `output_dir`.\n    save_on_each_node (`bool`, *optional*, defaults to `False`):\n        When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n        the main one.\n\n        This should not be activated when the different nodes use the same storage as the files will be saved with\n        the same names for each node.\n    no_cuda (`bool`, *optional*, defaults to `False`):\n        Whether to not use CUDA even when it is available or not.\n    seed (`int`, *optional*, defaults to 42):\n        Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n        [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n    data_seed (`int`, *optional*):\n        Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n        same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n        seed.\n    jit_mode_eval (`bool`, *optional*, defaults to `False`):\n        Whether or not to use PyTorch jit trace for inference.\n    use_ipex (`bool`, *optional*, defaults to `False`):\n        Use Intel extension for PyTorch when it is available. [IPEX\n        installation](https://github.com/intel/intel-extension-for-pytorch).\n    bf16 (`bool`, *optional*, defaults to `False`):\n        Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n        NVIDIA architecture or using CPU (no_cuda). This is an experimental API and it may change.\n    fp16 (`bool`, *optional*, defaults to `False`):\n        Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n    fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n        For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n        the [Apex documentation](https://nvidia.github.io/apex/amp).\n    fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n        This argument is deprecated. Use `half_precision_backend` instead.\n    half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n        The backend to use for mixed precision training. Must be one of `\"auto\", \"cuda_amp\", \"apex\", \"cpu_amp\"`.\n        `\"auto\"` will use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices\n        will force the requested backend.\n    bf16_full_eval (`bool`, *optional*, defaults to `False`):\n        Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n        metric values. This is an experimental API and it may change.\n    fp16_full_eval (`bool`, *optional*, defaults to `False`):\n        Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n        metric values.\n    tf32 (`bool`, *optional*):\n        Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n        on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n        the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation. This is an\n        experimental API and it may change.\n    local_rank (`int`, *optional*, defaults to -1):\n        Rank of the process during distributed training.\n    xpu_backend (`str`, *optional*):\n        The backend to use for xpu distributed training. Must be one of `\"mpi\"` or `\"ccl\"` or `\"gloo\"`.\n    tpu_num_cores (`int`, *optional*):\n        When training on TPU, the number of TPU cores (automatically passed by launcher script).\n    dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n        Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n        or not.\n    eval_steps (`int`, *optional*):\n        Number of update steps between two evaluations if `evaluation_strategy=\"steps\"`. Will default to the same\n        value as `logging_steps` if not set.\n    dataloader_num_workers (`int`, *optional*, defaults to 0):\n        Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n        main process.\n    past_index (`int`, *optional*, defaults to -1):\n        Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n        the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n        use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n        training step under the keyword argument `mems`.\n    run_name (`str`, *optional*):\n        A descriptor for the run. Typically used for [wandb](https://www.wandb.com/) and\n        [mlflow](https://www.mlflow.org/) logging.\n    disable_tqdm (`bool`, *optional*):\n        Whether or not to disable the tqdm progress bars and table of metrics produced by\n        [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n        set to warn or lower (default), `False` otherwise.\n    remove_unused_columns (`bool`, *optional*, defaults to `True`):\n        Whether or not to automatically remove the columns unused by the model forward method.\n\n        (Note that this behavior is not implemented for [`TFTrainer`] yet.)\n    label_names (`List[str]`, *optional*):\n        The list of keys in your dictionary of inputs that correspond to the labels.\n\n        Will eventually default to `[\"labels\"]` except if the model used is one of the `XxxForQuestionAnswering` in\n        which case it will default to `[\"start_positions\", \"end_positions\"]`.\n    load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n        Whether or not to load the best model found during training at the end of training.\n\n        <Tip>\n\n        When set to `True`, the parameters `save_strategy` needs to be the same as `evaluation_strategy`, and in\n        the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n\n        </Tip>\n\n    metric_for_best_model (`str`, *optional*):\n        Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n        models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`. Will\n        default to `\"loss\"` if unspecified and `load_best_model_at_end=True` (to use the evaluation loss).\n\n        If you set this value, `greater_is_better` will default to `True`. Don't forget to set it to `False` if\n        your metric is better when lower.\n    greater_is_better (`bool`, *optional*):\n        Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n        should have a greater metric or not. Will default to:\n\n        - `True` if `metric_for_best_model` is set to a value that isn't `\"loss\"` or `\"eval_loss\"`.\n        - `False` if `metric_for_best_model` is not set, or set to `\"loss\"` or `\"eval_loss\"`.\n    ignore_data_skip (`bool`, *optional*, defaults to `False`):\n        When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n        stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n        can take a long time) but will not yield the same results as the interrupted training would have.\n    sharded_ddp (`bool`, `str` or list of [`~trainer_utils.ShardedDDPOption`], *optional*, defaults to `False`):\n        Use Sharded DDP training from [FairScale](https://github.com/facebookresearch/fairscale) (in distributed\n        training only). This is an experimental feature.\n\n        A list of options along the following:\n\n        - `\"simple\"`: to use first instance of sharded DDP released by fairscale (`ShardedDDP`) similar to ZeRO-2.\n        - `\"zero_dp_2\"`: to use the second instance of sharded DPP released by fairscale (`FullyShardedDDP`) in\n          Zero-2 mode (with `reshard_after_forward=False`).\n        - `\"zero_dp_3\"`: to use the second instance of sharded DPP released by fairscale (`FullyShardedDDP`) in\n          Zero-3 mode (with `reshard_after_forward=True`).\n        - `\"offload\"`: to add ZeRO-offload (only compatible with `\"zero_dp_2\"` and `\"zero_dp_3\"`).\n\n        If a string is passed, it will be split on space. If a bool is passed, it will be converted to an empty\n        list for `False` and `[\"simple\"]` for `True`.\n    fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `False`):\n        Use PyTorch Distributed Parallel Training (in distributed training only).\n\n        A list of options along the following:\n\n        - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n        - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n        - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n          `\"shard_grad_op\"`).\n        - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n    fsdp_min_num_params (`int`, *optional*, defaults to `0`):\n        FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is passed).\n    deepspeed (`str` or `dict`, *optional*):\n        Use [Deepspeed](https://github.com/microsoft/deepspeed). This is an experimental feature and its API may\n        evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n        `ds_config.json`) or an already loaded json file as a `dict`\"\n    label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n        The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n        labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n        label_smoothing_factor/num_labels` respectively.\n    debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n        Enable one or more debug features. This is an experimental feature.\n\n        Possible options are:\n\n        - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n          the event\n        - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n\n        The options should be separated by whitespaces.\n    optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_hf\"`):\n        The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, adamw_anyprecision or adafactor.\n    optim_args (`str`, *optional*):\n        Optional arguments that are supplied to AnyPrecisionAdamW.\n    adafactor (`bool`, *optional*, defaults to `False`):\n        This argument is deprecated. Use `--optim adafactor` instead.\n    group_by_length (`bool`, *optional*, defaults to `False`):\n        Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n        padding applied and be more efficient). Only useful if applying dynamic padding.\n    length_column_name (`str`, *optional*, defaults to `\"length\"`):\n        Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n        than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n        instance of `Dataset`.\n    report_to (`str` or `List[str]`, *optional*, defaults to `\"all\"`):\n        The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n        `\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"`,`\"clearml\"` and `\"wandb\"`. Use `\"all\"` to report to\n        all integrations installed, `\"none\"` for no integrations.\n    ddp_find_unused_parameters (`bool`, *optional*):\n        When using distributed training, the value of the flag `find_unused_parameters` passed to\n        `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n    ddp_bucket_cap_mb (`int`, *optional*):\n        When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n    dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n        Whether you want to pin memory in data loaders or not. Will default to `True`.\n    skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n        Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n        down the training and evaluation speed.\n    push_to_hub (`bool`, *optional*, defaults to `False`):\n        Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n        `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n        will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n        [`~Trainer.save_model`] will also trigger a push.\n\n        <Tip warning={true}>\n\n        If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n        pushed.\n\n        </Tip>\n\n    resume_from_checkpoint (`str`, *optional*):\n        The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n        [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n    hub_model_id (`str`, *optional*):\n        The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n        which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n        for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n        `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n        name of `output_dir`.\n\n        Will default to the name of `output_dir`.\n    hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n        Defines the scope of what is pushed to the Hub and when. Possible values are:\n\n        - `\"end\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and a\n          draft of a model card when the [`~Trainer.save_model`] method is called.\n        - `\"every_save\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and\n          a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n          training, and in case the save are very frequent, a new push is only attempted if the previous one is\n          finished. A last push is made with the final model at the end of training.\n        - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n          last-checkpoint, allowing you to resume training easily with\n          `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n        - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n          folder (so you will get one checkpoint folder per folder in your final repository)\n\n    hub_token (`str`, *optional*):\n        The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n        `huggingface-cli login`.\n    hub_private_repo (`bool`, *optional*, defaults to `False`):\n        If True, the Hub repo will be set to private.\n    gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n        If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n    include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n        Whether or not the inputs will be passed to the `compute_metrics` function. This is intended for metrics\n        that need inputs, predictions and references for scoring calculation in Metric class.\n    auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n        Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n        CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n    full_determinism (`bool`, *optional*, defaults to `False`)\n        If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n        distributed training\n    torchdynamo (`str`, *optional*):\n        If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n        `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n    ray_scope (`str`, *optional*, defaults to `\"last\"`):\n        The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n        then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n        are also available. See the [Ray documentation](\n        https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n        more options.\n    ddp_timeout (`int`, *optional*, defaults to 1800):\n        The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n        performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n        (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n        information.\n    use_mps_device (`bool`, *optional*, defaults to `False`):\n        Whether to use Apple Silicon chip based `mps` device.\n    torch_compile (`bool`, *optional*, defaults to `False`):\n        Whether or not to compile the model using PyTorch 2.0\n        [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/) (requires a nighlty install of PyTorch).\n\n        If set, the backend will default to `\"inductor\"` (can be customized with `torch_compile_backend`) and the\n        mode will default to `\"default\"` (can be customized with `torch_compile_mode`).\n    torch_compile_backend (`str`, *optional*):\n        The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n\n        Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`, `\"nvfuser\"`, `\"aot_nvfuser\"`,\n        `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n    torch_compile_mode (`str`, *optional*):\n        The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n\n        Possible choices are `\"default\"`, `\"reduce-overhead\"` and `\"max-autotune\"`.\n\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/transformers/training_args.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     "},"metadata":{}}]},{"cell_type":"raw","source":"from transformers.trainer_utils import SchedulerType\nSchedulerType?","metadata":{}},{"cell_type":"raw","source":"!ls /kaggle/working","metadata":{"tags":[]}},{"cell_type":"code","source":"from pathlib import Path\ndata_dir = Path(\"/kaggle/workin\")\n#data_dir = Path(\"/tmp\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T14:59:01.214071Z","iopub.execute_input":"2023-06-26T14:59:01.214441Z","iopub.status.idle":"2023-06-26T14:59:01.219402Z","shell.execute_reply.started":"2023-06-26T14:59:01.214411Z","shell.execute_reply":"2023-06-26T14:59:01.218113Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model_name = tokenizer_repo.split(\"/\")[-1]\nmodel_name","metadata":{"execution":{"iopub.status.busy":"2023-06-26T14:59:01.968859Z","iopub.execute_input":"2023-06-26T14:59:01.969221Z","iopub.status.idle":"2023-06-26T14:59:01.975544Z","shell.execute_reply.started":"2023-06-26T14:59:01.969193Z","shell.execute_reply":"2023-06-26T14:59:01.974636Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'esperoberta-cased'"},"metadata":{}}]},{"cell_type":"code","source":"#model_name = \"esperoberta-cased\"\n#(data_dir/model_name).mkdir(exist_ok=True)\n# num_logging_times = 10\n\ntraining_args = TrainingArguments(\n    output_dir=data_dir/model_name,\n    overwrite_output_dir=True,\n    evaluation_strategy=\"steps\",\n    do_train=True,\n    do_eval=True,\n    no_cuda=not USE_CUDA,\n    #learning_rate=2e-5,\n    learning_rate=1e-4,\n    #num_train_epochs=1.0,\n    num_train_epochs=5.0,\n    #max_steps=MAX_STEPS,\n    #max_steps=21_000,\n    weight_decay=0.01,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    #push_to_hub=True,\n    #fp16=True,  # only on CUDA dMAX_STEPS\n    #logging_steps=MAX_STEPS//num_logging_times,\n    #save_steps=MAX_STEPS//num_logging_times,\n    fp16=FP16,\n    logging_steps=200,\n    save_steps=200,\n    eval_steps=200,\n    save_total_limit=4,\n    #load_best_model_at_end=True,\n    #remove_unused_columns=False,\n    remove_unused_columns=True,\n    #push_to_hub=tokenizer_repo,\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T15:02:16.566364Z","iopub.execute_input":"2023-06-26T15:02:16.566748Z","iopub.status.idle":"2023-06-26T15:02:16.584654Z","shell.execute_reply.started":"2023-06-26T15:02:16.566717Z","shell.execute_reply":"2023-06-26T15:02:16.583677Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Trainer","metadata":{"tags":[]}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=downsampled_dataset[\"train\"],\n    eval_dataset=downsampled_dataset[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-06-26T15:02:20.237021Z","iopub.execute_input":"2023-06-26T15:02:20.237380Z","iopub.status.idle":"2023-06-26T15:02:20.253824Z","shell.execute_reply.started":"2023-06-26T15:02:20.237350Z","shell.execute_reply":"2023-06-26T15:02:20.252767Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"Using cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Perplexity before training","metadata":{}},{"cell_type":"code","source":"import math\n\neval_results = trainer.evaluate()\nprint(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-26T15:02:23.444446Z","iopub.execute_input":"2023-06-26T15:02:23.444816Z","iopub.status.idle":"2023-06-26T15:02:37.305792Z","shell.execute_reply.started":"2023-06-26T15:02:23.444788Z","shell.execute_reply":"2023-06-26T15:02:37.304837Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 2954\n  Batch size = 64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='94' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [47/47 03:33]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"name":"stdout","text":">>> Perplexity: 2956.07\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**(?)** `16`? Number of steps, right?  \n**(R)** Yes, approximately: $\\frac{1000}{64} \\approx 16$","metadata":{}},{"cell_type":"code","source":"len(downsampled_dataset[\"test\"]) // BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2023-06-26T15:01:29.919010Z","iopub.execute_input":"2023-06-26T15:01:29.919378Z","iopub.status.idle":"2023-06-26T15:01:29.928454Z","shell.execute_reply.started":"2023-06-26T15:01:29.919348Z","shell.execute_reply":"2023-06-26T15:01:29.926939Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"92"},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2023-06-26T15:02:40.618960Z","iopub.execute_input":"2023-06-26T15:02:40.619312Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 292422\n  Num Epochs = 5\n  Instantaneous batch size per device = 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 22850\n  Number of trainable parameters = 109112880\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='418' max='22850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  418/22850 06:52 < 6:10:58, 1.01 it/s, Epoch 0.09/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>7.989300</td>\n      <td>7.873196</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>7.851000</td>\n      <td>7.817106</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 2954\n  Batch size = 64\nSaving model checkpoint to /kaggle/workin/esperoberta-cased/checkpoint-200\nConfiguration saved in /kaggle/workin/esperoberta-cased/checkpoint-200/config.json\nModel weights saved in /kaggle/workin/esperoberta-cased/checkpoint-200/pytorch_model.bin\ntokenizer config file saved in /kaggle/workin/esperoberta-cased/checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in /kaggle/workin/esperoberta-cased/checkpoint-200/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 2954\n  Batch size = 64\nSaving model checkpoint to /kaggle/workin/esperoberta-cased/checkpoint-400\nConfiguration saved in /kaggle/workin/esperoberta-cased/checkpoint-400/config.json\nModel weights saved in /kaggle/workin/esperoberta-cased/checkpoint-400/pytorch_model.bin\ntokenizer config file saved in /kaggle/workin/esperoberta-cased/checkpoint-400/tokenizer_config.json\nSpecial tokens file saved in /kaggle/workin/esperoberta-cased/checkpoint-400/special_tokens_map.json\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"#DEVICE = torch.device(\"cuda\") if USE_CUDA else torch.device(\"cpu\")\nDEVICE = torch.device(\"cpu\")","metadata":{}},{"cell_type":"raw","source":"def infer_viz(sample, k=5):\n    inputs = data_collator([sample])\n    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n    masked_text = tokenizer.decode(inputs[\"input_ids\"][0])\n    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n    #import ipdb; ipdb.set_trace()\n    clone = torch.clone(inputs[\"input_ids\"])\n    clone[0, mask_token_index] = inputs[\"labels\"][0, mask_token_index]\n    labels = tokenizer.decode(clone[0])\n\n\n    token_logits = model(**inputs).logits\n    mask_token_logits = token_logits[0, mask_token_index, :]\n    # Pick the <mask> candidates with the highest logits\n    top_k_tokens = torch.topk(mask_token_logits, k, dim=1).indices[0].tolist()\n\n    print(\"(masked)\")\n    print(f'\"{masked_text}\"', end=\"\\n\\n\")\n    \n    print(\"(gt)\")\n    print(f'\"{labels}\"', end=\"\\n\\n\")\n    \n    print(\"(preds)\")\n    for token in top_k_tokens:\n        print(f\"'{masked_text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")","metadata":{}},{"cell_type":"code","source":"ansi_code = {\n    #\"red\":    \"\\033[91m\",\n    \"red\":    \"\\x1b[91m\",\n    \"green\":  \"\\033[92m\",\n    \"yellow\": \"\\033[93m\",\n    \"blue\":   \"\\033[94m\",\n    \"pink\":   \"\\033[95m\",\n    \"teal\":   \"\\033[96m\",\n    \"grey\":   \"\\033[97m\",\n    #\"reset\":    \"\\x1b[0m\",\n    \"reset\":    \"\\033[0m\",\n}\n\ndef infer_viz2(sample, k=5):\n    inputs = data_collator([sample])\n    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n    masked_text = tokenizer.decode(inputs[\"input_ids\"][0])\n    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n    #import ipdb; ipdb.set_trace()\n    clone = torch.clone(inputs[\"input_ids\"])\n    clone[0, mask_token_index] = inputs[\"labels\"][0, mask_token_index]\n    labels = tokenizer.decode(clone[0])\n\n\n    token_logits = model(**inputs).logits\n    mask_token_logits = token_logits[0, mask_token_index, :]\n    # Pick the <mask> candidates with the highest logits\n    top_k_tokens = torch.topk(mask_token_logits, k, dim=1).indices[0].tolist()\n\n    #print(\"(masked)\")\n    #print(f'\"{masked_text}\"', end=\"\\n\\n\")\n    #\n    #print(\"(gt)\")\n    #print(f'\"{labels}\"', end=\"\\n\\n\")\n    \n    #print(\"(preds)\")\n    for token in top_k_tokens:\n        replacement = f'{ansi_code[\"red\"]}{tokenizer.decode([token])}{ansi_code[\"reset\"]}'\n        print(f\"'{masked_text.replace(tokenizer.mask_token, replacement)}'\")\n        print()","metadata":{},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"import random\n\nrandom.seed(42)\ni = random.randint(0, len(downsampled_dataset[\"test\"])-1)\nsample = downsampled_dataset[\"test\"][i]\nprint(sample)","metadata":{},"execution_count":44,"outputs":[{"name":"stdout","output_type":"stream","text":"{'input_ids': [11, 83, 11, 78, 324, 10227, 74, 11, 83, 4195, 11, 220, 213, 416, 11, 2314, 11, 1091, 11, 83, 11, 78, 11, 82, 16, 408, 2307, 379, 11410, 75, 11, 220, 389, 11, 82, 313, 2119, 17958, 11, 83, 30, 16512, 1739, 11, 220, 5728, 11, 69, 11, 82, 5805, 11, 83, 11, 82, 18, 2, 0, 4158, 11, 82, 1746, 11, 1579, 11, 73, 16, 213, 2865, 11, 83, 324, 213, 6064, 11, 1091, 11, 233, 11, 83, 404, 11, 361, 11, 220, 213, 12900, 11, 83, 11, 82, 18, 337, 684, 11, 24305, 11, 83, 11, 78, 2759, 4699, 237, 1236, 25540, 324, 4135, 11, 83, 4195, 11, 220, 391, 213, 403, 11, 69, 11, 82, 18, 2, 0, 954, 3293, 626, 395, 313, 2119], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [24, 25, 26, 27, 28, 29, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, None, None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 42, 43, 44, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, None, None, 0, 1, 2, 3, 4, 5], 'labels': [11, 83, 11, 78, 324, 10227, 74, 11, 83, 4195, 11, 220, 213, 416, 11, 2314, 11, 1091, 11, 83, 11, 78, 11, 82, 16, 408, 2307, 379, 11410, 75, 11, 220, 389, 11, 82, 313, 2119, 17958, 11, 83, 30, 16512, 1739, 11, 220, 5728, 11, 69, 11, 82, 5805, 11, 83, 11, 82, 18, 2, 0, 4158, 11, 82, 1746, 11, 1579, 11, 73, 16, 213, 2865, 11, 83, 324, 213, 6064, 11, 1091, 11, 233, 11, 83, 404, 11, 361, 11, 220, 213, 12900, 11, 83, 11, 82, 18, 337, 684, 11, 24305, 11, 83, 11, 78, 2759, 4699, 237, 1236, 25540, 324, 4135, 11, 83, 4195, 11, 220, 391, 213, 403, 11, 69, 11, 82, 18, 2, 0, 954, 3293, 626, 395, 313, 2119]}\n"}]},{"cell_type":"code","source":"infer_viz2(sample)","metadata":{},"execution_count":45,"outputs":[{"name":"stdout","output_type":"stream","text":"'\u001b[91m la\u001b[0mo'j kun\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m\u001b[91m la\u001b[0mo rigard'is la ĉe\u001b[91m la\u001b[0mest'ant'\u001b[91m la\u001b[0m\u001b[91m la\u001b[0mj'n, sed baldaŭ ek'reg\u001b[91m la\u001b[0mis ili'\u001b[91m la\u001b[0m mal'trankvil'o: Ramzes hav'is\u001b[91m la\u001b[0m'a'n vizaĝ'o'n.</s><s>Tio'\u001b[91m la\u001b[0m dir'int\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m, la\u001b[91m la\u001b[0m'o kun la\u001b[91m la\u001b[0m\u001b[91m la\u001b[0mant'ar'\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m'las'is\u001b[91m la\u001b[0m salon'o'n. La ĉef'pastr'o'j Herhor\u001b[91m la\u001b[0m Mefres kun tim'o rigard\u001b[91m la\u001b[0mis\u001b[91m la\u001b[0m la ali'a'n.</s><s>— Kial do ni\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m'\n\n\n\n'\u001b[91m.\u001b[0mo'j kun\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.\u001b[0mo rigard'is la ĉe\u001b[91m.\u001b[0mest'ant'\u001b[91m.\u001b[0m\u001b[91m.\u001b[0mj'n, sed baldaŭ ek'reg\u001b[91m.\u001b[0mis ili'\u001b[91m.\u001b[0m mal'trankvil'o: Ramzes hav'is\u001b[91m.\u001b[0m'a'n vizaĝ'o'n.</s><s>Tio'\u001b[91m.\u001b[0m dir'int\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m, la\u001b[91m.\u001b[0m'o kun la\u001b[91m.\u001b[0m\u001b[91m.\u001b[0mant'ar'\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m'las'is\u001b[91m.\u001b[0m salon'o'n. La ĉef'pastr'o'j Herhor\u001b[91m.\u001b[0m Mefres kun tim'o rigard\u001b[91m.\u001b[0mis\u001b[91m.\u001b[0m la ali'a'n.</s><s>— Kial do ni\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m'\n\n\n\n'\u001b[91m,\u001b[0mo'j kun\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m\u001b[91m,\u001b[0mo rigard'is la ĉe\u001b[91m,\u001b[0mest'ant'\u001b[91m,\u001b[0m\u001b[91m,\u001b[0mj'n, sed baldaŭ ek'reg\u001b[91m,\u001b[0mis ili'\u001b[91m,\u001b[0m mal'trankvil'o: Ramzes hav'is\u001b[91m,\u001b[0m'a'n vizaĝ'o'n.</s><s>Tio'\u001b[91m,\u001b[0m dir'int\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m, la\u001b[91m,\u001b[0m'o kun la\u001b[91m,\u001b[0m\u001b[91m,\u001b[0mant'ar'\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m'las'is\u001b[91m,\u001b[0m salon'o'n. La ĉef'pastr'o'j Herhor\u001b[91m,\u001b[0m Mefres kun tim'o rigard\u001b[91m,\u001b[0mis\u001b[91m,\u001b[0m la ali'a'n.</s><s>— Kial do ni\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m'\n\n\n\n'\u001b[91m de\u001b[0mo'j kun\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m\u001b[91m de\u001b[0mo rigard'is la ĉe\u001b[91m de\u001b[0mest'ant'\u001b[91m de\u001b[0m\u001b[91m de\u001b[0mj'n, sed baldaŭ ek'reg\u001b[91m de\u001b[0mis ili'\u001b[91m de\u001b[0m mal'trankvil'o: Ramzes hav'is\u001b[91m de\u001b[0m'a'n vizaĝ'o'n.</s><s>Tio'\u001b[91m de\u001b[0m dir'int\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m, la\u001b[91m de\u001b[0m'o kun la\u001b[91m de\u001b[0m\u001b[91m de\u001b[0mant'ar'\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m'las'is\u001b[91m de\u001b[0m salon'o'n. La ĉef'pastr'o'j Herhor\u001b[91m de\u001b[0m Mefres kun tim'o rigard\u001b[91m de\u001b[0mis\u001b[91m de\u001b[0m la ali'a'n.</s><s>— Kial do ni\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m'\n\n\n\n'\u001b[91m kaj\u001b[0mo'j kun\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0mo rigard'is la ĉe\u001b[91m kaj\u001b[0mest'ant'\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0mj'n, sed baldaŭ ek'reg\u001b[91m kaj\u001b[0mis ili'\u001b[91m kaj\u001b[0m mal'trankvil'o: Ramzes hav'is\u001b[91m kaj\u001b[0m'a'n vizaĝ'o'n.</s><s>Tio'\u001b[91m kaj\u001b[0m dir'int\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m, la\u001b[91m kaj\u001b[0m'o kun la\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0mant'ar'\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m'las'is\u001b[91m kaj\u001b[0m salon'o'n. La ĉef'pastr'o'j Herhor\u001b[91m kaj\u001b[0m Mefres kun tim'o rigard\u001b[91m kaj\u001b[0mis\u001b[91m kaj\u001b[0m la ali'a'n.</s><s>— Kial do ni\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m'\n\n\n"}]},{"cell_type":"code","source":"i = random.randint(0, len(downsampled_dataset[\"train\"])-1)\nsample = downsampled_dataset[\"train\"][i]\nprint(sample)","metadata":{},"execution_count":46,"outputs":[{"name":"stdout","output_type":"stream","text":"{'input_ids': [970, 1413, 16, 2183, 8679, 223, 1769, 5049, 255, 299, 7187, 2984, 2219, 914, 520, 2, 0, 394, 8679, 223, 1769, 5049, 255, 299, 11795, 30, 2408, 814, 1017, 223, 1769, 1204, 23604, 4111, 343, 213, 492, 1567, 330, 213, 5613, 13232, 1159, 443, 6017, 308, 17, 69, 4975, 28825, 256, 1943, 18, 337, 8679, 5183, 213, 20744, 8944, 377, 1680, 237, 213, 17943, 6575, 3169, 18, 788, 343, 14977, 213, 2873, 219, 223, 2765, 8331, 16, 11428, 1664, 2380, 538, 213, 12136, 223, 3142, 20844, 16088, 24278, 18, 337, 970, 2084, 2766, 23317, 757, 436, 213, 1771, 2741, 223, 308, 17, 69, 6861, 7645, 12920, 4445, 450, 18, 699, 28, 65, 3350, 213, 26450, 9970, 16, 14977, 213, 1715, 219, 223, 2592, 8786, 16, 5183, 213, 673], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [23, 24, 25, 26, 27, 28, 29, 29, 29, 30, 31, 31, 31, 31, 32, None, None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 7, 7, 8, 9, 9, 9, 10, 11, 12, 13, 14, 15, 16, 17, 17, 17, 18, 19, 20, 21, 22, 23, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 57, 58, 59, 60, 61, 62, 63, 63, 64, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 76, 77, 78, 79, 80, 81, 82, 83, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95], 'labels': [970, 1413, 16, 2183, 8679, 223, 1769, 5049, 255, 299, 7187, 2984, 2219, 914, 520, 2, 0, 394, 8679, 223, 1769, 5049, 255, 299, 11795, 30, 2408, 814, 1017, 223, 1769, 1204, 23604, 4111, 343, 213, 492, 1567, 330, 213, 5613, 13232, 1159, 443, 6017, 308, 17, 69, 4975, 28825, 256, 1943, 18, 337, 8679, 5183, 213, 20744, 8944, 377, 1680, 237, 213, 17943, 6575, 3169, 18, 788, 343, 14977, 213, 2873, 219, 223, 2765, 8331, 16, 11428, 1664, 2380, 538, 213, 12136, 223, 3142, 20844, 16088, 24278, 18, 337, 970, 2084, 2766, 23317, 757, 436, 213, 1771, 2741, 223, 308, 17, 69, 6861, 7645, 12920, 4445, 450, 18, 699, 28, 65, 3350, 213, 26450, 9970, 16, 14977, 213, 1715, 219, 223, 2592, 8786, 16, 5183, 213, 673]}\n"}]},{"cell_type":"code","source":"infer_viz2(sample)","metadata":{},"execution_count":48,"outputs":[{"name":"stdout","output_type":"stream","text":"'\u001b[91m la\u001b[0m uzas,\u001b[91m la\u001b[0m Traktato\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m\u001b[91m la\u001b[0mmalambiguigo).</s><s>\u001b[91m la\u001b[0m Traktato de Versajlo\u001b[91m la\u001b[0m france: Traité de\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m ) estis\u001b[91m la\u001b[0m plej grava el la packontraktoj\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m\u001b[91m la\u001b[0ma Mondmiliton al fino. La\u001b[91m la\u001b[0m finis la militan staton inter Germanio kaj la Aliancitaj ŝtatoj.\u001b[91m la\u001b[0m estis subskribita la 28an de junio 1919, ekzakte kvin jarojn\u001b[91m la\u001b[0m la murdo de arkiduko Franz Ferdinand.\u001b[91m la\u001b[0m\u001b[91m la\u001b[0m Centraj Potencoj sur la germana flanko de 1-\u001b[91m la\u001b[0m Mondmilito subskribis apartajn traktatojn. [8] Kvankam la armistico\u001b[91m la\u001b[0m subskribita la 11an de novembro 1918, finis la fa'\n\n\n\n'\u001b[91m.\u001b[0m uzas,\u001b[91m.\u001b[0m Traktato\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.\u001b[0mmalambiguigo).</s><s>\u001b[91m.\u001b[0m Traktato de Versajlo\u001b[91m.\u001b[0m france: Traité de\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m ) estis\u001b[91m.\u001b[0m plej grava el la packontraktoj\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.\u001b[0ma Mondmiliton al fino. La\u001b[91m.\u001b[0m finis la militan staton inter Germanio kaj la Aliancitaj ŝtatoj.\u001b[91m.\u001b[0m estis subskribita la 28an de junio 1919, ekzakte kvin jarojn\u001b[91m.\u001b[0m la murdo de arkiduko Franz Ferdinand.\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m Centraj Potencoj sur la germana flanko de 1-\u001b[91m.\u001b[0m Mondmilito subskribis apartajn traktatojn. [8] Kvankam la armistico\u001b[91m.\u001b[0m subskribita la 11an de novembro 1918, finis la fa'\n\n\n\n'\u001b[91m,\u001b[0m uzas,\u001b[91m,\u001b[0m Traktato\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m\u001b[91m,\u001b[0mmalambiguigo).</s><s>\u001b[91m,\u001b[0m Traktato de Versajlo\u001b[91m,\u001b[0m france: Traité de\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m ) estis\u001b[91m,\u001b[0m plej grava el la packontraktoj\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m\u001b[91m,\u001b[0ma Mondmiliton al fino. La\u001b[91m,\u001b[0m finis la militan staton inter Germanio kaj la Aliancitaj ŝtatoj.\u001b[91m,\u001b[0m estis subskribita la 28an de junio 1919, ekzakte kvin jarojn\u001b[91m,\u001b[0m la murdo de arkiduko Franz Ferdinand.\u001b[91m,\u001b[0m\u001b[91m,\u001b[0m Centraj Potencoj sur la germana flanko de 1-\u001b[91m,\u001b[0m Mondmilito subskribis apartajn traktatojn. [8] Kvankam la armistico\u001b[91m,\u001b[0m subskribita la 11an de novembro 1918, finis la fa'\n\n\n\n'\u001b[91m de\u001b[0m uzas,\u001b[91m de\u001b[0m Traktato\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m\u001b[91m de\u001b[0mmalambiguigo).</s><s>\u001b[91m de\u001b[0m Traktato de Versajlo\u001b[91m de\u001b[0m france: Traité de\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m ) estis\u001b[91m de\u001b[0m plej grava el la packontraktoj\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m\u001b[91m de\u001b[0ma Mondmiliton al fino. La\u001b[91m de\u001b[0m finis la militan staton inter Germanio kaj la Aliancitaj ŝtatoj.\u001b[91m de\u001b[0m estis subskribita la 28an de junio 1919, ekzakte kvin jarojn\u001b[91m de\u001b[0m la murdo de arkiduko Franz Ferdinand.\u001b[91m de\u001b[0m\u001b[91m de\u001b[0m Centraj Potencoj sur la germana flanko de 1-\u001b[91m de\u001b[0m Mondmilito subskribis apartajn traktatojn. [8] Kvankam la armistico\u001b[91m de\u001b[0m subskribita la 11an de novembro 1918, finis la fa'\n\n\n\n'\u001b[91m kaj\u001b[0m uzas,\u001b[91m kaj\u001b[0m Traktato\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0mmalambiguigo).</s><s>\u001b[91m kaj\u001b[0m Traktato de Versajlo\u001b[91m kaj\u001b[0m france: Traité de\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m ) estis\u001b[91m kaj\u001b[0m plej grava el la packontraktoj\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0ma Mondmiliton al fino. La\u001b[91m kaj\u001b[0m finis la militan staton inter Germanio kaj la Aliancitaj ŝtatoj.\u001b[91m kaj\u001b[0m estis subskribita la 28an de junio 1919, ekzakte kvin jarojn\u001b[91m kaj\u001b[0m la murdo de arkiduko Franz Ferdinand.\u001b[91m kaj\u001b[0m\u001b[91m kaj\u001b[0m Centraj Potencoj sur la germana flanko de 1-\u001b[91m kaj\u001b[0m Mondmilito subskribis apartajn traktatojn. [8] Kvankam la armistico\u001b[91m kaj\u001b[0m subskribita la 11an de novembro 1918, finis la fa'\n\n\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}