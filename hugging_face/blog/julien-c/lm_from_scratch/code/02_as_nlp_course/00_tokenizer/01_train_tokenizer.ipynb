{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa5afdb-82a8-4475-affe-aa3b212f102f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train According to [NLP Course from Hugging Face](https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2db77-e0d8-4069-90c5-68bb82aaed85",
   "metadata": {},
   "source": [
    "We'd like to build our VinaLM's tokenizer almost identical to both\n",
    "- ~PhoBERT's tokenizer~\n",
    "- RoBERTa's tokenizer (`\"roberta-base\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854b0a0a-225f-4c80-83b0-11623a23c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10c618f4-93bc-449a-9aa1-a72c84128897",
   "metadata": {},
   "source": [
    "models.BPE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36781cb-a1ff-4282-a5a9-09f19ecf5f15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 10:42:03.610680: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 10:42:06.354208: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/phunc20/.config/miniconda3/envs/py3.10/lib\n",
      "2023-06-26 10:42:06.354426: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/phunc20/.config/miniconda3/envs/py3.10/lib\n",
      "2023-06-26 10:42:06.354439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7df430-7ab9-4c0b-a527-6cfd16e864de",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_base_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85417147-9929-4113-ab00-fed80462f986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base_tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "raw",
   "id": "490e6d0f-9b4e-4bc3-b4f7-bc8c8e551e98",
   "metadata": {
    "tags": []
   },
   "source": [
    "models.BPE?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "695ed5bf-d4cc-408e-a31b-9ea3937dafb2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "Tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e8714c-469a-42be-a59a-efc1528fcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cased_bpe_tokenizer = Tokenizer(\n",
    "    models.BPE(unk_token=roberta_base_tokenizer.unk_token),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c53168-a53e-4a6a-a6df-2f739b3e8ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['add_special_tokens', 'add_tokens', 'decode', 'decode_batch', 'decoder', 'enable_padding', 'enable_truncation', 'encode', 'encode_batch', 'from_buffer', 'from_file', 'from_pretrained', 'from_str', 'get_vocab', 'get_vocab_size', 'id_to_token', 'model', 'no_padding', 'no_truncation', 'normalizer', 'num_special_tokens_to_add', 'padding', 'post_process', 'post_processor', 'pre_tokenizer', 'save', 'to_str', 'token_to_id', 'train', 'train_from_iterator', 'truncation']\n"
     ]
    }
   ],
   "source": [
    "print([s for s in dir(cased_bpe_tokenizer) if not s.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e50f90-06cf-4138-a30b-85d935bce8be",
   "metadata": {},
   "source": [
    "### Set `normalizer, pre_tokenizer, post_processor` etc.\n",
    "- `post_processor` to have, say, the final output of BOS, EOS tokens\n",
    "    - Simply choose `processors.RobertaProcessing`!\n",
    "    - `processors.ByteLevel` alone won't add BOS or EOS\n",
    "- `decoder`: `decoders.ByteLevel()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50457af6-5be5-4c8d-8d81-51eedef6ca57",
   "metadata": {},
   "source": [
    "Seems that we don't need any normalizer for Esperanto. Or, we just pick a neural one, e.g. `BerNormalizer`, which we try to ask to do nothing."
   ]
  },
  {
   "cell_type": "raw",
   "id": "598e662d-7433-427b-86e2-465756e004b4",
   "metadata": {},
   "source": [
    "normalizers.BertNormalizer?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b557afcb-a256-4a0f-97c5-cb8a0c7c21fe",
   "metadata": {},
   "source": [
    "normalizers.Normalizer?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "978f2179-958e-463e-817e-30a9bf8c772d",
   "metadata": {},
   "source": [
    "normalizers.Precompiled?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6a3206b-e349-42de-915a-0e4650dbf734",
   "metadata": {},
   "source": [
    "cased_bpe_tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.Replace(\"đ\", \"d\"),\n",
    "    normalizers.Replace(\"Đ\", \"D\"),\n",
    "    normalizers.NFD(),\n",
    "    #normalizers.Lowercase(),\n",
    "    normalizers.StripAccents(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bd7c8cc-6c61-4d5f-b96c-6875d8e9553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cased_bpe_tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.BertNormalizer(\n",
    "        lowercase=False,\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c49a8cd3-ca40-4287-8f81-8fcc67cae7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multaj homoj jam havas ĝin.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = \"Multaj homoj jam havas ĝin.\"\n",
    "cased_bpe_tokenizer.normalizer.normalize_str(sent2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0ca86f1-3311-48c6-9054-bf9c9a2da34e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "pre_tokenizers.ByteLevel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a9e983a-c626-4ecd-a09d-f37a3c6e6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "cased_bpe_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(\n",
    "    add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fc4b47c-da0f-40ba-84c5-40d9e192a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por Apple estas konata la apo Duolingo: por multaj lingvoj ĝi havas lingvan lernilon, nun ankaŭ por Esperanto.\n",
      "\n",
      "[('Por', (0, 3)), ('ĠApple', (3, 9)), ('Ġestas', (9, 15)), ('Ġkonata', (15, 22)), ('Ġla', (22, 25)), ('Ġapo', (25, 29)), ('ĠDuolingo', (29, 38)), (':', (38, 39)), ('Ġpor', (39, 43)), ('Ġmultaj', (43, 50)), ('Ġlingvoj', (50, 58)), ('ĠÄĿi', (58, 61)), ('Ġhavas', (61, 67)), ('Ġlingvan', (67, 75)), ('Ġlernilon', (75, 84)), (',', (84, 85)), ('Ġnun', (85, 89)), ('ĠankaÅŃ', (89, 95)), ('Ġpor', (95, 99)), ('ĠEsperanto', (99, 109)), ('.', (109, 110))]\n"
     ]
    }
   ],
   "source": [
    "sent3 = \"\"\"\\\n",
    "Por Apple estas konata la apo Duolingo:\n",
    "por multaj lingvoj ĝi havas lingvan lernilon,\n",
    "nun ankaŭ por Esperanto.\\\n",
    "\"\"\"\n",
    "normalized_sent3 = cased_bpe_tokenizer.normalizer.normalize_str(\n",
    "    sent3)\n",
    "pre_tokenized_sent3 = cased_bpe_tokenizer.pre_tokenizer.pre_tokenize_str(\n",
    "    normalized_sent3)\n",
    "print(normalized_sent3)\n",
    "print()\n",
    "print(pre_tokenized_sent3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8137454-ad34-4610-a605-f4511637046e",
   "metadata": {},
   "source": [
    "processors.RobertaProcessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b5f2920-db6e-47dd-927c-0896fd1cfa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token, sep_token_id = (\n",
    "    roberta_base_tokenizer.sep_token,\n",
    "    roberta_base_tokenizer.sep_token_id,\n",
    ")\n",
    "cls_token, cls_token_id = (\n",
    "    roberta_base_tokenizer.cls_token,\n",
    "    roberta_base_tokenizer.cls_token_id,\n",
    ")\n",
    "cased_bpe_tokenizer.post_processor = processors.RobertaProcessing(\n",
    "    sep=(sep_token, sep_token_id),\n",
    "    cls=(cls_token, cls_token_id),\n",
    "    #trim_offsets=True,\n",
    "    add_prefix_space=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183aaa8-e924-45c1-8c85-216a2d37c948",
   "metadata": {},
   "source": [
    "Lastly, `decoder`:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29754f6d-8ae2-4363-b65a-a755ab7468e3",
   "metadata": {},
   "source": [
    "decoders.ByteLevel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "351289ba-9999-4d9d-a05b-1458a06b4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "cased_bpe_tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7148524-ffd0-44f5-875d-63d94835e922",
   "metadata": {},
   "source": [
    "Recall that, for `\"robert-base\"` tokenizer,\n",
    "| Token Type | Token | Token ID |\n",
    "| --- | --- | --- |\n",
    "| `bos_token` | `\"<s>\"` | 0 |\n",
    "| `pad_token` | `\"<pad>\"` | 1 |\n",
    "| `eos_token` | `\"</s>\"` | 2 |\n",
    "| `unk_token` | `\"<unk>\"` | 3 |\n",
    "| `mask_token` | `\"<mask>\"` | 50264 |\n",
    "| `sep_token` | `\"</s>\"` | 2 |\n",
    "| `cls_token` | `\"<s>\"` | 0 |\n",
    "| `???` | `\"<\\|endoftext\\|>\"` | 50260 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04936b7a-7d72-415e-b9a4-c91d0f78bda0",
   "metadata": {},
   "source": [
    "## `Trainer` and Train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9da7a9b-354c-4eec-a134-b12892c59ea3",
   "metadata": {},
   "source": [
    "trainers.BpeTrainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b203ee2b-a7aa-41e3-bff5-31770384aed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base_tokenizer.special_tokens_map_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a59135e8-9993-4899-b7a6-041d33ce5dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import AddedToken\n",
    "AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b8d40e1-4d4c-4efa-a2a5-4fde81f3307d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "trainers.BpeTrainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77564180-97ca-4837-8d88-22dc49ef7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    show_progress=True,\n",
    "    # special_tokens=list(\n",
    "    #     roberta_base_tokenizer.special_tokens_map.values()),\n",
    "    #special_tokens=[\"<unk>\", \"<pad>\", \"<cls>\", \"<sep>\", \"<mask>\"],\n",
    "    #special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"],\n",
    "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"],\n",
    "    # special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\",\n",
    "    #     roberta_base_tokenizer.special_tokens_map_extended[\"mask_token\"],\n",
    "    # ],\n",
    "    #special_tokens=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bec948e0-bd43-43cd-b0f9-ff786bdfe349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False),\n",
       " AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False),\n",
       " AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False),\n",
       " AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False),\n",
       " AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "185dc43d-d509-4349-94b5-cdb903139283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e4d300c-03f3-4c70-b09d-9018ff1f649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../../../data/\")\n",
    "assert data_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c232dfc-1960-470f-8f39-dcf28e40695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_paths = [\n",
    "    data_dir/\"oscar.eo.txt\"\n",
    "]\n",
    "assert all(p.exists() for p in text_paths), \"some path doesn't exist! Check again, please!\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "143f1976-f98d-436a-8f31-08e5997b4262",
   "metadata": {},
   "source": [
    "cased_bpe_tokenizer.train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c874d374-a22c-4b1e-a01a-e03c4705294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cased_bpe_tokenizer.train(\n",
    "    [str(p) for p in text_paths],\n",
    "    #[str(p) for p in toy_text_paths],\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9d44d31-ff23-4a42-9f51-1feb2e518835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=24, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = cased_bpe_tokenizer.encode(sent3)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b053bb4-dbb6-4188-9f58-d050faf6a5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Por',\n",
       " 'ĠApple',\n",
       " 'Ġestas',\n",
       " 'Ġkonata',\n",
       " 'Ġla',\n",
       " 'Ġapo',\n",
       " 'ĠDuolingo',\n",
       " ':',\n",
       " 'Ġpor',\n",
       " 'Ġmultaj',\n",
       " 'Ġlingvoj',\n",
       " 'ĠÄĿi',\n",
       " 'Ġhavas',\n",
       " 'Ġlingvan',\n",
       " 'Ġlern',\n",
       " 'ilon',\n",
       " ',',\n",
       " 'Ġnun',\n",
       " 'ĠankaÅŃ',\n",
       " 'Ġpor',\n",
       " 'ĠEsperanto',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a70915c-11d6-4dfe-88f1-70b23e242ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1949,\n",
       " 18453,\n",
       " 265,\n",
       " 1792,\n",
       " 213,\n",
       " 2324,\n",
       " 10652,\n",
       " 30,\n",
       " 289,\n",
       " 824,\n",
       " 1151,\n",
       " 433,\n",
       " 581,\n",
       " 8438,\n",
       " 901,\n",
       " 2240,\n",
       " 16,\n",
       " 584,\n",
       " 474,\n",
       " 289,\n",
       " 488,\n",
       " 18,\n",
       " 2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "382bfc96-b389-45d8-a935-331cb402db93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = cased_bpe_tokenizer.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ff14af9-ecfd-4642-acd0-163810b77a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<pad>', '</s>', '<unk>', '<mask>', '!', '\"']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cased_bpe_tokenizer.id_to_token(i) for i in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94c4c688-9024-4593-a0ad-c0348d059685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ãģĹ', 'ĠkaÅĿitan', 'migrado', '130', 'Ġlegiti', 'Ġplibonigoj']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cased_bpe_tokenizer.id_to_token(vocab_size-i) for i in range(1,7)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "956aad31-c989-4685-a45a-44fa4b5c2948",
   "metadata": {},
   "source": [
    "print(dir(cased_bpe_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e7cc2-44c7-4063-86e2-9faeb669200a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db64931f-fdb2-477f-8219-1a41cac575d4",
   "metadata": {},
   "source": [
    "### Making It into A Fast Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a970c1b-b44c-4f6e-9e33-d6262422f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    PreTrainedTokenizerFast,\n",
    "    RobertaForMaskedLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f836271f-0bae-44fe-823a-4f2a90d412fa",
   "metadata": {},
   "source": [
    "roberta_mlm = RobertaForMaskedLM.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de0023c6-7fb8-47ee-beb6-dac9ab5458af",
   "metadata": {},
   "source": [
    "print(dir(roberta_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c22cb6c-433d-404e-843d-702cad0c596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base_tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b756a4f6-2f4f-4e11-9a3e-d70851087592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roberta-base': 512,\n",
       " 'roberta-large': 512,\n",
       " 'roberta-large-mnli': 512,\n",
       " 'distilroberta-base': 512,\n",
       " 'roberta-base-openai-detector': 512,\n",
       " 'roberta-large-openai-detector': 512}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base_tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "573a1ccf-ac84-49af-b84a-c9450d49efe3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "PreTrainedTokenizerFast?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19bbdde4-cb58-4306-87ab-ef8cdf44c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "RobertaTokenizerFast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b610debd-863e-4629-ae40-cc484ab714db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_cased_bpe_tokenizer = RobertaTokenizerFast(\n",
    "    tokenizer_object=cased_bpe_tokenizer,\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "# fast_cased_bpe_tokenizer = PreTrainedTokenizerFast(\n",
    "#     tokenizer_object=cased_bpe_tokenizer,\n",
    "#     unk_token=roberta_base_tokenizer.unk_token,\n",
    "#     pad_token=roberta_base_tokenizer.pad_token,\n",
    "#     cls_token=roberta_base_tokenizer.cls_token,\n",
    "#     sep_token=roberta_base_tokenizer.sep_token,\n",
    "#     mask_token=roberta_base_tokenizer.mask_token,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4374cff8-177e-44c5-a7bc-d68df3bda325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_cased_bpe_tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "698433f3-f916-4ac5-9ccd-02fc8ac50c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_cased_bpe_tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cd2740a-b076-4601-aaee-ffd2e38d722c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_cased_bpe_tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b68a5d0-2d8e-415d-b2b9-9fece4a4a299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'bos_token', '<s>')\n",
      "(2, 'eos_token', '</s>')\n",
      "(3, 'unk_token', '<unk>')\n",
      "(2, 'sep_token', '</s>')\n",
      "(1, 'pad_token', '<pad>')\n",
      "(0, 'cls_token', '<s>')\n",
      "(4, 'mask_token', '<mask>')\n"
     ]
    }
   ],
   "source": [
    "for special_token_type, special_token in fast_cased_bpe_tokenizer.special_tokens_map.items():\n",
    "    special_token_id = fast_cased_bpe_tokenizer.convert_tokens_to_ids(\n",
    "        special_token)\n",
    "    print((special_token_id, special_token_type, special_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f966e2a-8bf4-4849-b666-a03de1268a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1949, 18453, 265, 1792, 213, 2324, 10652, 30, 289, 824, 1151, 433, 581, 8438, 901, 2240, 16, 584, 474, 289, 488, 18, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = fast_cased_bpe_tokenizer(sent3)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bdf64671-b386-416f-ac04-0683f9d7ee65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Por',\n",
       " 'ĠApple',\n",
       " 'Ġestas',\n",
       " 'Ġkonata',\n",
       " 'Ġla',\n",
       " 'Ġapo',\n",
       " 'ĠDuolingo',\n",
       " ':',\n",
       " 'Ġpor',\n",
       " 'Ġmultaj',\n",
       " 'Ġlingvoj',\n",
       " 'ĠÄĿi',\n",
       " 'Ġhavas',\n",
       " 'Ġlingvan',\n",
       " 'Ġlern',\n",
       " 'ilon',\n",
       " ',',\n",
       " 'Ġnun',\n",
       " 'ĠankaÅŃ',\n",
       " 'Ġpor',\n",
       " 'ĠEsperanto',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edeaf688-d2d7-432c-8792-88764aab9f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59cd7875-e568-4860-8dd8-0715e2dc2939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_cased_bpe_tokenizer.backend_tokenizer is cased_bpe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5118ffcf-bdd2-4fbc-b26a-a151caa25019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_cased_bpe_tokenizer.backend_tokenizer == cased_bpe_tokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2fa6c24-3e12-4b6b-8f9c-6e7723d80d38",
   "metadata": {},
   "source": [
    "tokenizer_dir = (\n",
    "    repo_dir/\n",
    "    \"viet_message/16-VinaLM_from_scratch/01_RoBERTa-like/data/\"\n",
    "    \"vina-cased_tokenizer\"\n",
    ")\n",
    "print(f'{tokenizer_dir.exists() = }')\n",
    "print(f'{tokenizer_dir.parent.exists() = }')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6730e659-d90e-427c-8b41-47752fb1ebef",
   "metadata": {},
   "source": [
    "fast_cased_bpe_tokenizer.save_pretrained(\n",
    "    save_directory=tokenizer_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7df88-4ee2-4ed5-8330-3931807e6f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c0a5ba7-02f1-4905-908b-f1275902dca2",
   "metadata": {},
   "source": [
    "### Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "213d1db3-e707-487d-afa9-e4a5f879cc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/phunc20/esperoberta-cased/commit/491d7128d0b603b50bf06fd098661609b0d78938', commit_message='Upload tokenizer', commit_description='', oid='491d7128d0b603b50bf06fd098661609b0d78938', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_repo = \"phunc20/esperoberta-cased\"\n",
    "fast_cased_bpe_tokenizer.push_to_hub(hub_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d96ab-9a57-4833-9a53-24a3e9bebc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20098f54-718c-4ae9-96b4-5c02882a8473",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reload to Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc06e08-e239-43dc-908d-a5046cedee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d34fdb72-5969-4915-8124-b8a511eec397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b717a7f29b42ce95eee538c51f2a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/311 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b30d528574c4b6a8b9aeb30fa31e6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0f1928f7264a3c94e0f44066726d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d093a2eb2ec42a2869c6e845f2d94d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52314349b07244799a07b95cbd383e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='phunc20/esperoberta-cased', vocab_size=30000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    hub_repo,\n",
    ")\n",
    "reloaded_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d42d41a2-9e81-4ff6-a8bd-4bc964e4a26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79d3a6fc-c928-4c15-9ed9-a21fe2be56db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multaj homoj jam havas ĝin.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cee32ee3-7df0-4d73-822b-1c05bbf73fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Por Apple estas konata la apo Duolingo:\\npor multaj lingvoj ĝi havas lingvan lernilon,\\nnun ankaŭ por Esperanto.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "839ea9ae-c8c7-4dd7-a083-aee677112be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 7628, 768, 600, 581, 644, 18, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding2 = reloaded_tokenizer(sent2)\n",
    "encoding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b87b7566-3507-4fb7-a59f-cdefe27ea55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Multaj', 'Ġhomoj', 'Ġjam', 'Ġhavas', 'ĠÄĿin', '.', '</s>']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding2.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1f104aa-a4ca-4bef-853f-f1c514e3822f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Multaj homoj jam havas ĝin.</s>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizer.convert_tokens_to_string(encoding2.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5281e1-0e5e-43e2-aa38-3f2b5de54172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9a82fff-2607-45cc-b4e2-71559eedd707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1949, 18453, 265, 1792, 213, 2324, 10652, 30, 289, 824, 1151, 433, 581, 8438, 901, 2240, 16, 584, 474, 289, 488, 18, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding3 = reloaded_tokenizer(sent3)\n",
    "encoding3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c3c6cd6-2d61-4624-aa55-12b4a9e083d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Por',\n",
       " 'ĠApple',\n",
       " 'Ġestas',\n",
       " 'Ġkonata',\n",
       " 'Ġla',\n",
       " 'Ġapo',\n",
       " 'ĠDuolingo',\n",
       " ':',\n",
       " 'Ġpor',\n",
       " 'Ġmultaj',\n",
       " 'Ġlingvoj',\n",
       " 'ĠÄĿi',\n",
       " 'Ġhavas',\n",
       " 'Ġlingvan',\n",
       " 'Ġlern',\n",
       " 'ilon',\n",
       " ',',\n",
       " 'Ġnun',\n",
       " 'ĠankaÅŃ',\n",
       " 'Ġpor',\n",
       " 'ĠEsperanto',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding3.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f62bb18-7cff-4b52-8b36-cb433778fc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Por Apple estas konata la apo Duolingo: por multaj lingvoj ĝi havas lingvan lernilon, nun ankaŭ por Esperanto.</s>'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizer.convert_tokens_to_string(encoding3.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be7572-8e02-4fb4-9699-e8bf89f02b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
